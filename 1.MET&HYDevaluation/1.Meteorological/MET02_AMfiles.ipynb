{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import lmom as lmom\n",
    "import MetFunctions as mf\n",
    "import MetFunctions01 as mf1\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc892951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddSavg(am1, S1_savg, S2_savg, C_savg):\n",
    "    conditionsALL = [am1.season=='DJF', am1.season=='JJA', am1.season=='MAM', am1.season=='SON']\n",
    "\n",
    "    choicesS1 = [S1_savg['DJF'], S1_savg['JJA'], S1_savg['MAM'], S1_savg['SON']]\n",
    "    choicesS2 = [S2_savg['DJF'], S2_savg['JJA'], S2_savg['MAM'], S2_savg['SON']]\n",
    "    choicesC = [C_savg['DJF'], C_savg['JJA'], C_savg['MAM'], C_savg['SON']]\n",
    "\n",
    "    am1['S1savg'] = np.select(conditionsALL, choicesS1, default='ERROR').astype(float)\n",
    "    am1['S2savg'] = np.select(conditionsALL, choicesS2, default='ERROR').astype(float)\n",
    "    am1['Csavg'] = np.select(conditionsALL, choicesC, default='ERROR').astype(float)\n",
    "    am1['S1/S1savg'] = am1.P_S1rol / am1.S1savg\n",
    "    am1['S2/S2savg'] = am1.P_S2rol / am1.S2savg\n",
    "    am1['C/Csavg'] = am1.Conf / am1.Csavg\n",
    "    return am1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccfc1c",
   "metadata": {},
   "source": [
    "Read each file and get the AM sets per file, combine them per confluence (50,000 peaks) per each set according to the contidion (S1, S2, C), and store the dataframes for later\n",
    "Loops:\n",
    "\n",
    "    C - Confluences\n",
    "\n",
    "    j - files\n",
    "    \n",
    "    i - ensembles per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd417b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RAIN \n",
    "conf = pd.read_csv('Confluences_wnames.csv', delimiter=';')\n",
    "w = 5\n",
    "# c = 0\n",
    "am1 = pd.DataFrame()\n",
    "am2 = pd.DataFrame()\n",
    "am3 = pd.DataFrame()\n",
    "savg_S1 = 0\n",
    "savg_S2 = 0\n",
    "savg_C = 0\n",
    "count = 0\n",
    "\n",
    "files = 2\n",
    "ensemble = 20\n",
    "\n",
    "for c in range(2): #len(conf)\n",
    "    sv1 =  r\"D:\\AMrain\\C%d_\" %c\n",
    "    for j in range(files):\n",
    "        path1 = r\"D:\\TimeSeries\\ds%d\" %j\n",
    "        for i in range(ensemble):\n",
    "            path2 = r\"_%draina.npy\" %i\n",
    "            path = path1 + path2\n",
    "            ds_rain = np.load(path, allow_pickle=True).item()\n",
    "            am1_, am2_, am3_, savg_S1_, savg_S2_, savg_C_ = mf1.AM(ds_rain, conf, c, w)\n",
    "            am1 = pd.concat([am1, am1_])\n",
    "            am2 = pd.concat([am2, am2_])\n",
    "            am3 = pd.concat([am3, am3_])\n",
    "            savg_S1 += savg_S1_\n",
    "            savg_S2 += savg_S2_\n",
    "            savg_C += savg_C_\n",
    "            count += 1\n",
    "    \n",
    "    #from all the loops, get am1, am2, am3, savg_S1, savg_S2, savg_C combinado de los 50,000 anos\n",
    "    S1_savg = savg_S1 / count\n",
    "    S2_savg = savg_S2 / count\n",
    "    C_savg = savg_C / count\n",
    "    \n",
    "    AM1 = AddSavg(am1, S1_savg, S2_savg, C_savg)\n",
    "    AM2 = AddSavg(am2, S1_savg, S2_savg, C_savg)\n",
    "    AM3 = AddSavg(am3, S1_savg, S2_savg, C_savg)\n",
    "    #now we want to sabe the 3 data frames per confluence\n",
    "    AM1.to_pickle(sv1 + \"AM1.pkl\")\n",
    "    AM2.to_pickle(sv1 + \"AM2.pkl\")\n",
    "    AM3.to_pickle(sv1 + \"AM3.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAIN 1 day \n",
    "conf = pd.read_csv('Confluences_wnames.csv', delimiter=';')\n",
    "w = 1 #5 do it with 1 to compare the differeces between 1 and 5 days\n",
    "# c = 0\n",
    "am1 = pd.DataFrame()\n",
    "am2 = pd.DataFrame()\n",
    "am3 = pd.DataFrame()\n",
    "savg_S1 = 0\n",
    "savg_S2 = 0\n",
    "savg_C = 0\n",
    "count = 0\n",
    "\n",
    "files = 2\n",
    "ensemble = 20\n",
    "\n",
    "\n",
    "for j in range(23, 25):  # Maybe only 5 files? -->10,000\n",
    "    path1 = r\"F:\\TimeSeries\\ds%d\" %j\n",
    "    print(f'Starting File {j}')\n",
    "    for i in range(0, 20):\n",
    "        print(f'Ensemble {i}')\n",
    "        path2 = r\"_%draina.npy\" %i\n",
    "        path = path1 + path2\n",
    "        ds_rain = np.load(path, allow_pickle=True).item()\n",
    "        for c in range(0, len(conf)): #len(conf)\n",
    "            sv =  r\"F:\\AMrain1d\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i \n",
    "            am1_, am2_, am3_, savg_S1_, savg_S2_, savg_C_ = mf1.AM(ds_rain, conf, c, w)\n",
    "            am1_.to_pickle(sv + \"AM1.pkl\")\n",
    "            am2_.to_pickle(sv + \"AM2.pkl\")\n",
    "            am3_.to_pickle(sv + \"AM3.pkl\")\n",
    "            savg_S1_.to_pickle(sv + \"savg_S1.pkl\")\n",
    "            savg_S2_.to_pickle(sv + \"savg_S2.pkl\")\n",
    "            savg_C_.to_pickle(sv + \"savg_C.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNOW\n",
    "conf = pd.read_csv('Confluences_wnames.csv', delimiter=';')\n",
    "w = 5\n",
    "# c = 0\n",
    "am1 = pd.DataFrame()\n",
    "am2 = pd.DataFrame()\n",
    "am3 = pd.DataFrame()\n",
    "savg_S1 = 0\n",
    "savg_S2 = 0\n",
    "savg_C = 0\n",
    "count = 0\n",
    "\n",
    "files = 25\n",
    "ensemble = 20\n",
    "\n",
    "for j in range(20, 25):  #10\n",
    "    print(f'Starting File {j}')\n",
    "    path1 = r\"F:\\TimeSeries\\ds%d\" %j\n",
    "    for i in range(0, 20): #20\n",
    "        print(f'Ensemble {i}')\n",
    "        path2 = r\"_%dsnow.npy\" %i\n",
    "        path = path1 + path2\n",
    "        ds_rain = np.load(path, allow_pickle=True).item()\n",
    "        for c in range(0, len(conf)): #len(conf)\n",
    "            sv =  r\"F:\\AMsnow\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i \n",
    "#             sv = r\"P:\\11206883-006-dar-cloud-computing\\RhineFiles\\MET\\AMsnow\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i\n",
    "            am1_, am2_, am3_, savg_S1_, savg_S2_, savg_C_ = mf1.AM(ds_rain, conf, c, w)\n",
    "            am1_.to_pickle(sv + \"AM1.pkl\")\n",
    "            am2_.to_pickle(sv + \"AM2.pkl\")\n",
    "            am3_.to_pickle(sv + \"AM3.pkl\")\n",
    "            savg_S1_.to_pickle(sv + \"savg_S1.pkl\")\n",
    "            savg_S2_.to_pickle(sv + \"savg_S2.pkl\")\n",
    "            savg_C_.to_pickle(sv + \"savg_C.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dar-cloud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dba0fec6c0a5dc3650c48ce5794cc777babb314a0f31f50d55fe76ea6fe6645b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
