{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff786f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gp\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import lmom as lmom\n",
    "import MetFunctions as mf1\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f37e4",
   "metadata": {},
   "source": [
    "Merge all the separate files of AM (25 files x 20 ensembles) to have 1 dataframe of 50,000 peaks\n",
    "\n",
    "Add the seasonal average of the streams\n",
    "\n",
    "Save the new merged file: r\"F:\\AMrain\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i + \"AM3.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538dd4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddSavg(am1, S1_savg, S2_savg, C_savg):\n",
    "    conditionsALL = [am1.season=='DJF', am1.season=='JJA', am1.season=='MAM', am1.season=='SON']\n",
    "\n",
    "    choicesS1 = [S1_savg['DJF'], S1_savg['JJA'], S1_savg['MAM'], S1_savg['SON']]\n",
    "    choicesS2 = [S2_savg['DJF'], S2_savg['JJA'], S2_savg['MAM'], S2_savg['SON']]\n",
    "    choicesC = [C_savg['DJF'], C_savg['JJA'], C_savg['MAM'], C_savg['SON']]\n",
    "\n",
    "    am1['S1savg'] = np.select(conditionsALL, choicesS1, default='ERROR').astype(float)\n",
    "    am1['S2savg'] = np.select(conditionsALL, choicesS2, default='ERROR').astype(float)\n",
    "    am1['Csavg'] = np.select(conditionsALL, choicesC, default='ERROR').astype(float)\n",
    "    am1['S1/S1savg'] = am1.P_S1rol / am1.S1savg\n",
    "    am1['S2/S2savg'] = am1.P_S2rol / am1.S2savg\n",
    "    am1['C/Csavg'] = am1.Conf / am1.Csavg\n",
    "    return am1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAIN 5d\n",
    "conf = pd.read_csv('Confluences_wnames.csv', delimiter=';')\n",
    "w = 5\n",
    "# c = 0\n",
    "\n",
    "\n",
    "# files = 2\n",
    "# ensemble = 20\n",
    "\n",
    "for c in range(0, len(conf)): #0 , 5   len(conf)\n",
    "    print(f'Starting {c} at {str(datetime.now().time())}')\n",
    "    count = 0\n",
    "    am1 = pd.DataFrame()\n",
    "    am2 = pd.DataFrame()\n",
    "    am3 = pd.DataFrame()\n",
    "    savg_S1 = 0\n",
    "    savg_S2 = 0\n",
    "    savg_C = 0\n",
    "\n",
    "    sv = r\"F:\\MET\\AMmet\\C%d_\" %c + r\"rain_\"\n",
    "    for j in range(0, 25):  # 0, 25 \n",
    "        for i in range(0, 20): #0, 20\n",
    "             #len(conf)\n",
    "            op =  r\"F:\\MET\\AMrain5d\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i  \n",
    "            am1_ = pd.read_pickle(op + \"AM1.pkl\")\n",
    "            am2_ = pd.read_pickle(op + \"AM2.pkl\")\n",
    "            am3_ = pd.read_pickle(op + \"AM3.pkl\")\n",
    "            savg_S1_ = pd.read_pickle(op + \"savg_S1.pkl\")\n",
    "            savg_S2_ = pd.read_pickle(op + \"savg_S2.pkl\")\n",
    "            savg_C_ = pd.read_pickle(op + \"savg_C.pkl\")\n",
    "            am1 = pd.concat([am1, am1_])\n",
    "            am2 = pd.concat([am2, am2_])\n",
    "            am3 = pd.concat([am3, am3_])\n",
    "            savg_S1 += savg_S1_\n",
    "            savg_S2 += savg_S2_\n",
    "            savg_C += savg_C_\n",
    "            count += 1\n",
    "    #from all the loops, get am1, am2, am3, savg_S1, savg_S2, savg_C combinado de los 50,000 anos\n",
    "    S1_savg = savg_S1 / count\n",
    "    S2_savg = savg_S2 / count\n",
    "    C_savg = savg_C / count\n",
    "        \n",
    "    AM1 = AddSavg(am1, S1_savg, S2_savg, C_savg)\n",
    "    AM2 = AddSavg(am2, S1_savg, S2_savg, C_savg)\n",
    "    AM3 = AddSavg(am3, S1_savg, S2_savg, C_savg)\n",
    "#     now we want to save the 3 data frames per confluence\n",
    "    AM1.to_pickle(sv + \"AM1.pkl\")\n",
    "    AM2.to_pickle(sv + \"AM2.pkl\")\n",
    "    AM3.to_pickle(sv + \"AM3.pkl\")\n",
    "    print(f'Finishing {c} at {str(datetime.now().time())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78647c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAIN 1d\n",
    "conf = pd.read_csv('Confluences_wnames.csv', delimiter=';')\n",
    "w = 5\n",
    "# c = 0\n",
    "\n",
    "\n",
    "# files = 2\n",
    "# ensemble = 20\n",
    "\n",
    "for c in range(0, len(conf)): #0 , 5   len(conf)\n",
    "    print(f'Starting {c} at {str(datetime.now().time())}')\n",
    "    count = 0\n",
    "    am1 = pd.DataFrame()\n",
    "    am2 = pd.DataFrame()\n",
    "    am3 = pd.DataFrame()\n",
    "    savg_S1 = 0\n",
    "    savg_S2 = 0\n",
    "    savg_C = 0\n",
    "\n",
    "    sv = r\"F:\\MET\\AMmet\\C%d_\" %c + r\"rain1d_\"\n",
    "    for j in range(0, 25):  # 0, 25 \n",
    "        for i in range(0, 20): #0, 20\n",
    "             #len(conf)\n",
    "            op =  r\"F:\\MET\\AMrain1d\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i  \n",
    "            am1_ = pd.read_pickle(op + \"AM1.pkl\")\n",
    "            am2_ = pd.read_pickle(op + \"AM2.pkl\")\n",
    "            am3_ = pd.read_pickle(op + \"AM3.pkl\")\n",
    "            savg_S1_ = pd.read_pickle(op + \"savg_S1.pkl\")\n",
    "            savg_S2_ = pd.read_pickle(op + \"savg_S2.pkl\")\n",
    "            savg_C_ = pd.read_pickle(op + \"savg_C.pkl\")\n",
    "            am1 = pd.concat([am1, am1_])\n",
    "            am2 = pd.concat([am2, am2_])\n",
    "            am3 = pd.concat([am3, am3_])\n",
    "            savg_S1 += savg_S1_\n",
    "            savg_S2 += savg_S2_\n",
    "            savg_C += savg_C_\n",
    "            count += 1\n",
    "    #from all the loops, get am1, am2, am3, savg_S1, savg_S2, savg_C combinado de los 50,000 anos\n",
    "    S1_savg = savg_S1 / count\n",
    "    S2_savg = savg_S2 / count\n",
    "    C_savg = savg_C / count\n",
    "        \n",
    "    AM1 = AddSavg(am1, S1_savg, S2_savg, C_savg)\n",
    "    AM2 = AddSavg(am2, S1_savg, S2_savg, C_savg)\n",
    "    AM3 = AddSavg(am3, S1_savg, S2_savg, C_savg)\n",
    "#     now we want to save the 3 data frames per confluence\n",
    "    AM1.to_pickle(sv + \"AM1.pkl\")\n",
    "    AM2.to_pickle(sv + \"AM2.pkl\")\n",
    "    AM3.to_pickle(sv + \"AM3.pkl\")\n",
    "    print(f'Finishing {c} at {str(datetime.now().time())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snow\n",
    "conf = pd.read_csv('Confluences_wnames.csv', delimiter=';')\n",
    "w = 5\n",
    "# c = 0\n",
    "\n",
    "\n",
    "# files = 2\n",
    "# ensemble = 20\n",
    "\n",
    "for c in range(0, len(conf)): #len(conf)\n",
    "    print(f'starting c{c}')\n",
    "    count = 0\n",
    "    am1 = pd.DataFrame()\n",
    "    am2 = pd.DataFrame()\n",
    "    am3 = pd.DataFrame()\n",
    "    savg_S1 = 0\n",
    "    savg_S2 = 0\n",
    "    savg_C = 0\n",
    "\n",
    "    sv = r\"F:\\MET\\AMmet\\C%d_\" %c + r\"snow_\"\n",
    "    for j in range(0, 25):  # 0, 25 \n",
    "        for i in range(0, 20): #0, 20\n",
    "             #len(conf)\n",
    "            op =  r\"F:\\MET\\AMsnow\\C%d_\" %c + r\"%d_\" %j + r\"%d\" %i  \n",
    "            am1_ = pd.read_pickle(op + \"AM1.pkl\")\n",
    "            am2_ = pd.read_pickle(op + \"AM2.pkl\")\n",
    "            am3_ = pd.read_pickle(op + \"AM3.pkl\")\n",
    "            savg_S1_ = pd.read_pickle(op + \"savg_S1.pkl\")\n",
    "            savg_S2_ = pd.read_pickle(op + \"savg_S2.pkl\")\n",
    "            savg_C_ = pd.read_pickle(op + \"savg_C.pkl\")\n",
    "            am1 = pd.concat([am1, am1_])\n",
    "            am2 = pd.concat([am2, am2_])\n",
    "            am3 = pd.concat([am3, am3_])\n",
    "            savg_S1 += savg_S1_\n",
    "            savg_S2 += savg_S2_\n",
    "            savg_C += savg_C_\n",
    "            count += 1\n",
    "    #from all the loops, get am1, am2, am3, savg_S1, savg_S2, savg_C combinado de los 50,000 anos\n",
    "    S1_savg = savg_S1 / count\n",
    "    S2_savg = savg_S2 / count\n",
    "    C_savg = savg_C / count\n",
    "        \n",
    "    AM1 = AddSavg(am1, S1_savg, S2_savg, C_savg)\n",
    "    AM2 = AddSavg(am2, S1_savg, S2_savg, C_savg)\n",
    "    AM3 = AddSavg(am3, S1_savg, S2_savg, C_savg)\n",
    "#     now we want to save the 3 data frames per confluence\n",
    "    AM1.to_pickle(sv + \"AM1.pkl\")\n",
    "    AM2.to_pickle(sv + \"AM2.pkl\")\n",
    "    AM3.to_pickle(sv + \"AM3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc31991",
   "metadata": {},
   "outputs": [],
   "source": [
    "op =  r\"F:\\MET\\AMrain\\C0_\" + r\"20_\"  + r\"10\"   \n",
    "am1_ = pd.read_pickle(op + \"AM1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34977fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "op =  r\"F:\\MET\\AMrain\\C0_\" + r\"20_\"  + r\"5\"   \n",
    "\n",
    "C0_20_5savg_C = pd.read_pickle(op + \"savg_C.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dar-cloud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dba0fec6c0a5dc3650c48ce5794cc777babb314a0f31f50d55fe76ea6fe6645b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
